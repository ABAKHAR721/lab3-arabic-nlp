{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXK0Dc17V2Nv"
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers transformers beautifulsoup4 nltk datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SiAtkkvaV3zB"
   },
   "outputs": [],
   "source": [
    "import requests, re, time, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import torch, torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from transformers import AutoTokenizer, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download only what's needed (avoid broken punkt_tab)\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Arabic stopwords\n",
    "arabic_stopwords = set(stopwords.words(\"arabic\"))\n",
    "\n",
    "# Semantic sentence model\n",
    "sbert_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "reference = \"هذا النص يتحدث عن التعليم في الوطن العربي\"\n",
    "ref_embedding = sbert_model.encode(reference, convert_to_tensor=True)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745074698287,
     "user": {
      "displayName": "Abdssamad Abkhar",
      "userId": "15335981712636211350"
     },
     "user_tz": -60
    },
    "id": "hrKEtnVPWWZM"
   },
   "outputs": [],
   "source": [
    "def get_article_links(base_url, pages=2):\n",
    "    links = []\n",
    "    for page in range(1, pages + 1):\n",
    "        url = f\"{base_url}?page={page}\"\n",
    "        res = requests.get(url)\n",
    "        soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            if \"/news/\" in a[\"href\"] and not a[\"href\"].startswith(\"https\"):\n",
    "                links.append(\"https://www.aljazeera.net\" + a[\"href\"])\n",
    "    return list(set(links))\n",
    "\n",
    "def extract_text(url):\n",
    "    try:\n",
    "        soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "        return \" \".join(p.get_text() for p in soup.find_all(\"p\")).strip()\n",
    "    except:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1745074699969,
     "user": {
      "displayName": "Abdssamad Abkhar",
      "userId": "15335981712636211350"
     },
     "user_tz": -60
    },
    "id": "bFNWQVPMWZsU"
   },
   "outputs": [],
   "source": [
    "def semantic_score(text):\n",
    "    emb = sbert_model.encode(text, convert_to_tensor=True)\n",
    "    sim = util.cos_sim(emb, ref_embedding).item()\n",
    "    return round(max(0.0, min(sim * 10, 10.0)), 2)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r\"[^\\u0600-\\u06FF\\s]\", \"\", text)  # Keep Arabic\n",
    "    tokens = text.split()  # Simple whitespace tokenizer\n",
    "    tokens = [t for t in tokens if t not in arabic_stopwords and len(t) > 2]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1745074701561,
     "user": {
      "displayName": "Abdssamad Abkhar",
      "userId": "15335981712636211350"
     },
     "user_tz": -60
    },
    "id": "wPVPo-coWbjL"
   },
   "outputs": [],
   "source": [
    "def scrape_articles():\n",
    "    links = get_article_links(\"https://www.aljazeera.net/news/\")\n",
    "    data = []\n",
    "    for link in links[:20]:\n",
    "        text = extract_text(link)\n",
    "        if len(text.split()) > 50:\n",
    "            score = semantic_score(text)\n",
    "            data.append({\"Text\": text, \"Score\": score})\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"tokens\"] = df[\"Text\"].apply(preprocess)\n",
    "    df.to_csv(\"arabic_dataset.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1EoFp2aXBsD"
   },
   "outputs": [],
   "source": [
    "data = scrape_articles()\n",
    "data[\"tokens\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "34a643d9599443a9a71ef1c5cba35a25",
      "a2d7579030bb472088fe2abe6c1b7adc",
      "2cf1fdb222374f3eb31bb43e9d133bc4",
      "df64cf92570b4cdab5acb5b67335fe90",
      "57ce242c802d4d4bb02d2218dc5a42dc",
      "7db8573e019f453b8dea6e3d2f1f95b8",
      "ef182785555043bd940a4f63a06e4da5",
      "a7d9b9128cf14f6f807b8aa18a0f005a",
      "dc32c9e64cc841c9bf18e26069af0e5c",
      "917c22ff98d64b09bb171682aee92a67",
      "859b9d23ede945aabd104ab5cd4d937d",
      "8e6c2aa3d0f84e18b0ceaae20330f0cc",
      "465ae5300c534be5a967d917ce2c3de0",
      "2471be2c6d5841ff89d15fb83497170b",
      "37ebdaadbafd408e8b9f494832a06d80",
      "86551df4195c4b2db4e7355155e8f78b",
      "f99a409c174642b4a6b4ded0e7e8f4fe",
      "ee737402de624eab96f2e433992b79cf",
      "13f72e40573f43208789306b8b15a757",
      "d577e988dd6f478c8d4e7acbfd284332",
      "3f255ce853604d6d8dedd8aff5daefa4",
      "41d9757ce1274992894ec1f01b23a182",
      "17ac19ad0a824f0aa3f4b04f1d6834d1",
      "f5aea849e3d6442a92bf2fa20c0f1f6e",
      "03cb1f36e2464b7b87b55d30cdab7d40",
      "156338ed8d5a42a9940d94928095386c",
      "646f57de9645486486ff0d6f27764d39",
      "7616df2f524c4e319df3bd6651eecdd5",
      "051a2fbe162d4bb5800a3883df1979f5",
      "25d2d77877ba4c0caf46b556b69fce61",
      "bfc0aff1df6440259a7927684b5c9472",
      "9bead89b5f9649a485b4f1f56013c867",
      "a9eeaec6daba4da9812540faa6bf76d5",
      "2a3c7a3e977249bcbc7e15f499e767f2",
      "99098e27d4de4302acc352ee3626c224",
      "65b914492ada49278228f848b3b91b7b",
      "359cb9b50880423195e3bd603e11c9c5",
      "eed838cc18d342cb8e0b0e8818b0e2f3",
      "7c4dc4cb64c34e2f8112cd1cd311bc39",
      "b9388beb7991462e93f335d08ef19454",
      "212a5f19cf234a48ad39ec88623b84ab",
      "76d3e89094fb46878616a5a0200b4e86",
      "8d8e324479214882bb2ee5a4d7ec9c13",
      "f38af3c496f4445c850698011c962d46"
     ]
    },
    "executionInfo": {
     "elapsed": 1825,
     "status": "ok",
     "timestamp": 1745074717522,
     "user": {
      "displayName": "Abdssamad Abkhar",
      "userId": "15335981712636211350"
     },
     "user_tz": -60
    },
    "id": "2MuceEkHWc7A",
    "outputId": "0956d1bd-5345-4049-9c44-f90797b21470"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a643d9599443a9a71ef1c5cba35a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/62.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e6c2aa3d0f84e18b0ceaae20330f0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/491 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ac19ad0a824f0aa3f4b04f1d6834d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/334k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3c7a3e977249bcbc7e15f499e767f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n",
    "data[\"input_ids\"] = data[\"tokens\"].apply(lambda x: tokenizer.encode(\" \".join(x), padding=\"max_length\", max_length=100, truncation=True))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"input_ids\"].tolist(), data[\"Score\"].tolist(), test_size=0.2)\n",
    "\n",
    "class ArabicDataset(Dataset):\n",
    "    def __init__(self, encodings, scores):\n",
    "        self.encodings = encodings\n",
    "        self.scores = scores\n",
    "    def __len__(self): return len(self.encodings)\n",
    "    def __getitem__(self, idx): return torch.tensor(self.encodings[idx]), torch.tensor(self.scores[idx], dtype=torch.float)\n",
    "\n",
    "train_loader = DataLoader(ArabicDataset(X_train, y_train), batch_size=2)\n",
    "test_loader = DataLoader(ArabicDataset(X_test, y_test), batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745074719791,
     "user": {
      "displayName": "Abdssamad Abkhar",
      "userId": "15335981712636211350"
     },
     "user_tz": -60
    },
    "id": "90WCMdgBWg35"
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, rnn_type, vocab_size, embed_dim=128, hidden_dim=64, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        rnn_cls = {\"rnn\": nn.RNN, \"gru\": nn.GRU, \"lstm\": nn.LSTM}[rnn_type]\n",
    "        self.rnn = rnn_cls(embed_dim, hidden_dim, batch_first=True, bidirectional=bidirectional)\n",
    "        self.bidirectional = bidirectional\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, h = self.rnn(x)\n",
    "\n",
    "        # Handle LSTM hidden state tuple (h, c)\n",
    "        if isinstance(h, tuple):\n",
    "            h = h[0]\n",
    "\n",
    "        # Bidirectional: concatenate forward and backward hidden states\n",
    "        if self.bidirectional:\n",
    "            h_out = torch.cat((h[0], h[1]), dim=1)  # shape [batch, hidden*2]\n",
    "        else:\n",
    "            h_out = h[-1]  # shape [batch, hidden]\n",
    "\n",
    "        return self.fc(h_out).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1745074724904,
     "user": {
      "displayName": "Abdssamad Abkhar",
      "userId": "15335981712636211350"
     },
     "user_tz": -60
    },
    "id": "Tgb0GFZoWh_D"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(rnn_type, bidirectional=False):\n",
    "    print(f\"\\n==> Training {rnn_type.upper()} {'Bi' if bidirectional else ''}RNN\")\n",
    "    model = RNNModel(rnn_type, tokenizer.vocab_size, bidirectional=bidirectional).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(3):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            preds = model(X_batch)\n",
    "            loss = loss_fn(preds.view(-1), y_batch.view(-1))  # Fix shape mismatch\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    preds, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            preds.extend(y_pred.cpu().view(-1).numpy())\n",
    "            actuals.extend(y_batch.numpy())\n",
    "\n",
    "    print(\"MSE:\", round(mean_squared_error(actuals, preds), 4))\n",
    "    print(\"R²:\", round(r2_score(actuals, preds), 4))\n",
    "\n",
    "    # BLEU evaluation with rounding\n",
    "    bleu_scores = [\n",
    "        sentence_bleu([[str(round(act, 1))]], str(round(pred, 1)),\n",
    "                      smoothing_function=SmoothingFunction().method4)\n",
    "        for pred, act in zip(preds, actuals)\n",
    "    ]\n",
    "    print(\"BLEU Score:\", round(sum(bleu_scores) / len(bleu_scores), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 809,
     "status": "ok",
     "timestamp": 1745074729869,
     "user": {
      "displayName": "Abdssamad Abkhar",
      "userId": "15335981712636211350"
     },
     "user_tz": -60
    },
    "id": "5V6KuQroXzzw",
    "outputId": "93c82642-b2f2-4939-b0be-a5a4b012d158"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Training RNN RNN\n",
      "MSE: 0.6279\n",
      "R²: -1.6907\n",
      "BLEU Score: 0.0\n",
      "\n",
      "==> Training GRU RNN\n",
      "MSE: 1.5393\n",
      "R²: -5.5962\n",
      "BLEU Score: 0.0\n",
      "\n",
      "==> Training LSTM RNN\n",
      "MSE: 5.7165\n",
      "R²: -23.4969\n",
      "BLEU Score: 0.0\n",
      "\n",
      "==> Training LSTM BiRNN\n",
      "MSE: 2.0246\n",
      "R²: -7.6761\n",
      "BLEU Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(\"rnn\")\n",
    "train_and_evaluate(\"gru\")\n",
    "train_and_evaluate(\"lstm\")\n",
    "train_and_evaluate(\"lstm\", bidirectional=True)  # BiLSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318,
     "referenced_widgets": [
      "a0ab256ca5ac4ae392e0f700fabf0ee0",
      "2cc74649b5e942e9a4184f92d9067198",
      "f517a9ed590c4c439b3cc92678db8023",
      "d181edcb4d9c4de984cb4e9139173e20",
      "af4d464b940b469389d330976e62724c",
      "1161db94f3354246987996e22fe4f1f8",
      "8d1dc9ababfa4e7598be771aeb4c0862",
      "cd3b146f04384e4fb0219019312a9d99",
      "3d293ba544ec49ea802a9a5fbae60708",
      "0fe61a013b254b20a93655f8ce788b3f",
      "4a012c3d7d5e41069e1ea88b7ce5a972",
      "62e9ade4218c4e49a75a0de6414d7b37",
      "92f613cf93864bef9732c6171b44048b",
      "8949d72955cc4085b18afb718768c113",
      "b5c573c93d4c4e4788ff6c307e22b4d1",
      "16307c884781403a85817b9ef3a1855f",
      "b426d9b2302d4604b296eb472e3a1556",
      "beaeb25effed4d299622c2d5a472f1e8",
      "6f8fe90abc54421b802b4ed62512d60b",
      "f8bc72ffebff4c78b9bde9b27e25cab0",
      "e8b7f13fdc574143a190d749f840801c",
      "5ba407d2bc98416fae745eb88899dabf",
      "87dfd69866e24629b931cc83509baa24",
      "b81f9cb4558140528d9e2175cc379633",
      "60888e3b35254ee8a3e69413814cb105",
      "00d28396dce446bfa371ce6d27974929",
      "f6edc51701c543bb95a1716c10815b35",
      "2d73285c00ef4ff9b5ce0f2375696934",
      "8296b4d3544c427690280ca7e480e72e",
      "7917f78623474a779231c950557b5e5b",
      "3fa72929115e4868a1c89ace12875b69",
      "dcc473ecd3b5457e997dee66c950fc80",
      "550cc29d2bc548a581d57be3cf401e60",
      "17ee4ed913d846febca758973832aa1e",
      "77d8b36cf1ed48eca4dabc915e6d0d47",
      "3b88d5e4bd5d436ab0acb27bed6df2f1",
      "73bfad19ccdf4182832acd2ffa85b06d",
      "4cef52dfdeaa4640badd1c0ebc35916c",
      "18c9c1915aca4e878ec39413a7b07cfc",
      "8eb2b71e3034494bb4a52ad642fefbe9",
      "f8916bbfe9f3451583f8457f01a52982",
      "5503a517decf40adaf178d1ceb81fdcb",
      "301398ba4f034cd48cd38c2e9da9a0e2",
      "518d3e72933b401aa6d7931002217b4b",
      "eeea1db5ebf84d24952bd50499ab1daf",
      "90a76a2550b946a39755d2f26292c9ae",
      "c768bafe575840a3be848fbe57233879",
      "30b479facdbe4d808c83b8041fab847c",
      "71bab85f85a84c93bffdad984e22dd8d",
      "f5b2e931d3eb48ad9fd6acafc00be810",
      "5d4c5c827abc4aa98e4c80623462f591",
      "5c923de765fb4cf4baaab4e790742705",
      "614d6e8f2f584523b8e10dbbbd808eed",
      "d2e5058b797a46018f0f2950459e431c",
      "87687f05ac4b4d558315da8ab7841304"
     ]
    },
    "executionInfo": {
     "elapsed": 6445,
     "status": "ok",
     "timestamp": 1745074741731,
     "user": {
      "displayName": "Abdssamad Abkhar",
      "userId": "15335981712636211350"
     },
     "user_tz": -60
    },
    "id": "oaM9BkuMWjom",
    "outputId": "4f1edb82-3ed1-48b4-ba93-e6f1c0f149ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Arabic Text Generation with GPT-2 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ab256ca5ac4ae392e0f700fabf0ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e9ade4218c4e49a75a0de6414d7b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.50M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87dfd69866e24629b931cc83509baa24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/4.52M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ee4ed913d846febca758973832aa1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeea1db5ebf84d24952bd50499ab1daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/553M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Paragraph:\n",
      "\n",
      "الذكاء الاصطناعي سيساهم في تطوير التعليم في الأردن \" .وأضاف أن \" هناك تحديات أخرى تواجه التعليم في الأردن ، منها نقص الكوادر البشرية المؤهلة ، وعدم وجود خطط واضحة في مجال التعليم ، وعدم وجود خطة واضحة في مجال التعليم \" .وأوضح أن \" وجود تعليم مرتبط بالطلبة ، سيؤدي إلى زيادة معدلات التسرب المدرسي ، وبالتالي إلى تقليل نسبة التسرب المدرسي ، وكذلك زيادة عدد الطلبة ، وبالتالي زيادة نسبة الالتحاق بالمدارس\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Arabic Text Generation with GPT-2 ---\")\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"aubmindlab/aragpt2-base\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"aubmindlab/aragpt2-base\").to(device)\n",
    "gpt2_model.eval()\n",
    "\n",
    "prompt = \"الذكاء الاصطناعي سيساهم في تطوير التعليم\"\n",
    "inputs = gpt2_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated = gpt2_model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=80,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=gpt2_tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated Paragraph:\\n\")\n",
    "print(gpt2_tokenizer.decode(generated[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1163,
     "status": "ok",
     "timestamp": 1745074747310,
     "user": {
      "displayName": "Abdssamad Abkhar",
      "userId": "15335981712636211350"
     },
     "user_tz": -60
    },
    "id": "TFZNciDSYKYF",
    "outputId": "236df970-5a3f-4d93-cf5f-1c4c7749aaf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1:\n",
      "الذكاء الاصطناعي سيساهم في تطوير التعليم عن بعد ، وسيتمكن هذا النظام من إنشاء دورات تعليمية جديدة عبر الإنترنت ، في الوقت نفسه سيساهم الذكاء الاصطناعي في تطوير المدارس .وكان الذكاء الاصطناعي قد بدأ في تطوير نظام ذكاء اصطناعي مخصص لإدارة التعليم عن بعد ، وقد بدأ بتطويره في عام 2013 .\n",
      "\n",
      "Sample 2:\n",
      "الذكاء الاصطناعي سيساهم في تطوير التعليم العالي في الأردن بشكل عام والتعليم العالي بوجه خاص .\n",
      "\n",
      "Sample 3:\n",
      "الذكاء الاصطناعي سيساهم في تطوير التعليم ورفع كفاءته ، خصوصا في ظل ما تشهده العديد من البلدان من انتشار التقنيات الحديثة ، الأمر الذي يستدعي وضع استراتيجية وطنية وطنية لتطوير التعليم ، خاصة أنه القطاع الذي لا يزال يفتقر إلى الكفاءات والمهارات اللازمة في هذا الشأن .وأكد أن هذا الأمر يتطلب من الجامعات الخاصة ، تكوين وتدريب وتأهيل كوادرها الوطنية ، من خلال تطوير برامجها في هذا الشأن ، وكذلك إيجاد البرامج الأكاديمية المتخصصة لت\n"
     ]
    }
   ],
   "source": [
    "generated = gpt2_model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=80,\n",
    "    num_return_sequences=3,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.9,\n",
    "    pad_token_id=gpt2_tokenizer.eos_token_id\n",
    ")\n",
    "for i, sample in enumerate(generated):\n",
    "    print(f\"\\nSample {i+1}:\\n{gpt2_tokenizer.decode(sample, skip_special_tokens=True)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "each-HCUj7KK"
   },
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1745074773447,
     "user": {
      "displayName": "Abdssamad Abkhar",
      "userId": "15335981712636211350"
     },
     "user_tz": -60
    },
    "id": "qQVJpWxAj9c-"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"arabic_dataset.csv\")\n",
    "with open(\"gpt2_arabic_train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in df[\"Text\"]:\n",
    "        f.write(line.strip() + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 773,
     "status": "ok",
     "timestamp": 1745074788754,
     "user": {
      "displayName": "Abdssamad Abkhar",
      "userId": "15335981712636211350"
     },
     "user_tz": -60
    },
    "id": "oGoiWSwvj-Uz",
    "outputId": "a3b02109-bf4d-4bdd-8a72-7247420c9429"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "model_name = \"aubmindlab/aragpt2-base\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT2 has no pad token\n",
    "\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"gpt2_arabic_train.txt\",\n",
    "    block_size=128\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Language modeling, not masked\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 704,
     "status": "ok",
     "timestamp": 1745074800651,
     "user": {
      "displayName": "Abdssamad Abkhar",
      "userId": "15335981712636211350"
     },
     "user_tz": -60
    },
    "id": "kzDgS3MxkDY8"
   },
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "executionInfo": {
     "elapsed": 99045,
     "status": "ok",
     "timestamp": 1745074908362,
     "user": {
      "displayName": "Abdssamad Abkhar",
      "userId": "15335981712636211350"
     },
     "user_tz": -60
    },
    "id": "HggiHbkwkF9r",
    "outputId": "dd65c2c9-ced4-4564-ebb7-5d183e403f12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlegendsit1234\u001b[0m (\u001b[33mlegendsit1234-university-abdelmalek-essaadi\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250419_150122-63h4lf9q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/legendsit1234-university-abdelmalek-essaadi/huggingface/runs/63h4lf9q' target=\"_blank\">./gpt2_arabic_finetuned</a></strong> to <a href='https://wandb.ai/legendsit1234-university-abdelmalek-essaadi/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/legendsit1234-university-abdelmalek-essaadi/huggingface' target=\"_blank\">https://wandb.ai/legendsit1234-university-abdelmalek-essaadi/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/legendsit1234-university-abdelmalek-essaadi/huggingface/runs/63h4lf9q' target=\"_blank\">https://wandb.ai/legendsit1234-university-abdelmalek-essaadi/huggingface/runs/63h4lf9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=6.208026529947917, metrics={'train_runtime': 97.6374, 'train_samples_per_second': 1.506, 'train_steps_per_second': 0.768, 'total_flos': 9602482176000.0, 'train_loss': 6.208026529947917, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_arabic_finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8261,
     "status": "ok",
     "timestamp": 1745074929151,
     "user": {
      "displayName": "Abdssamad Abkhar",
      "userId": "15335981712636211350"
     },
     "user_tz": -60
    },
    "id": "KH6Y8v2TkaZp",
    "outputId": "278e4de5-4ac4-4386-d459-9ad8bef3a954"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gpt2_arabic_finetuned/tokenizer_config.json',\n",
       " './gpt2_arabic_finetuned/special_tokens_map.json',\n",
       " './gpt2_arabic_finetuned/vocab.json',\n",
       " './gpt2_arabic_finetuned/merges.txt',\n",
       " './gpt2_arabic_finetuned/added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./gpt2_arabic_finetuned\")\n",
    "tokenizer.save_pretrained(\"./gpt2_arabic_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3552,
     "status": "ok",
     "timestamp": 1745074935330,
     "user": {
      "displayName": "Abdssamad Abkhar",
      "userId": "15335981712636211350"
     },
     "user_tz": -60
    },
    "id": "a1k56vDJkhSz",
    "outputId": "3cd5236e-4ec9-4b2c-f5ab-8434e2897a09"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أهمية التعليم في العصر الحديث ، وفي ظل ما يشهده العالم من تغيرات عميقة على مختلف الصعد السياسية والاقتصادية والاجتماعية والثقافية، فضلا عن التحديات التي تواجه المجتمع الدولي بأسره، خاصة فيما يتعلق بقضايا اللاجئين الفلسطينيين، الذين يعيشون في الأراضي المحتلة منذ عام 1948، إذ لا تزال أعدادهم تتزايد، إلا أن هناك تحديات كبيرة تواجههم، تتمثل في استمرار الاحتلال الإسرائيلي في انتهاك القانون الدولي الإنساني، وانتهاك حقوق الإنسان، إضافة إلى الانتهاكات الجسيمة لحقوق الإنسان الأخرى، بما فيها جرائم الحرب والجرائم ضد\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"./gpt2_arabic_finetuned\", tokenizer=tokenizer)\n",
    "\n",
    "prompt = \"أهمية التعليم في العصر الحديث\"\n",
    "outputs = generator(prompt, max_length=100, num_return_sequences=1)\n",
    "\n",
    "print(outputs[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2840,
     "status": "ok",
     "timestamp": 1745075076223,
     "user": {
      "displayName": "Abdssamad Abkhar",
      "userId": "15335981712636211350"
     },
     "user_tz": -60
    },
    "id": "rh6nhyUJlGyK",
    "outputId": "1aca219e-b53a-438d-d8bb-41c006c9e9a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1:\n",
      "أهمية التعليم في العصر الحديث، وذلك من أجل أن تكون المناهج التعليمية على مستوى عال من الجودة ، وأن يكون لها دور فعال في تحقيق أهداف التنمية الاقتصادية والاجتماعية، بالإضافة إلى أهمية الدور الذي تقوم به وزارة التربية والتعليم ممثلة في المديرية العامة للتربية والتعليم بمحافظة جنوب سيناء، حيث تعمل الوزارة بالتنسيق مع مديرية التربية والتعليم بالمحافظة على إعداد مناهج تعليمية تتناسب مع احتياجات الطلاب وأولياء الأمور واحتياجاتهم المستقبلية، كما تسعى الوزارة إلى تطوير المناهج التعليمية بما يتناسب مع متطلبات المرحلة الراهنة والمستقبلية، وتعمل الوزارة على توفير كافة الإمكانيات والوسائل اللازمة لتطوير المناهج التعليمية وفقا للمعايير الدولية التي وضعتها المنظمة الدولية للتربية والثقافة والعلوم \" اليونسكو \"�\n",
      "\n",
      "Sample 2:\n",
      "أهمية التعليم في العصر الحديث، وذلك من أجل أن تكون المناهج التعليمية على مستوى عال من الجودة ، وأن يكون لها دور فعال في تحقيق أهداف التنمية الاقتصادية والاجتماعية، بالإضافة إلى أهمية الدور الذي تقوم به وزارة التربية والتعليم ممثلة في المديرية العامة للتربية والتعليم بمحافظة جنوب سيناء، حيث تعمل الوزارة بالتنسيق مع مديرية التربية والتعليم بالمحافظة على إعداد مناهج تعليمية تتناسب مع احتياجات الطلاب وأولياء الأمور واحتياجاتهم المستقبلية، كما تسعى الوزارة إلى تطوير المناهج التعليمية بما يتناسب مع متطلبات المرحلة الراهنة والمستقبلية، وتعمل الوزارة على توفير كافة الإمكانيات والوسائل اللازمة لتطوير المناهج التعليمية وفقا للمعايير الدولية التي وضعتها المنظمة الدولية للتربية والثقافة والعلوم \" اليونيسكو \"\n",
      "\n",
      "Sample 3:\n",
      "أهمية التعليم في العصر الحديث، وذلك من أجل أن تكون المناهج التعليمية على مستوى عال من الجودة ، وأن يكون لها دور فعال في تحقيق أهداف التنمية الاقتصادية والاجتماعية، بالإضافة إلى أهمية الدور الذي تقوم به وزارة التربية والتعليم ممثلة في المديرية العامة للتربية والتعليم بمحافظة جنوب سيناء، حيث تعمل الوزارة بالتنسيق مع مديرية التربية والتعليم بالمحافظة على إعداد مناهج تعليمية تتناسب مع احتياجات الطلاب وأولياء الأمور واحتياجاتهم المستقبلية، كما تسعى الوزارة إلى تطوير المناهج التعليمية بما يتناسب مع متطلبات المرحلة الراهنة والمستقبلية، وتعمل الوزارة على توفير كافة الإمكانيات والوسائل اللازمة لتطوير المناهج التعليمية وفقا للمعايير الدولية التي وضعتها المنظمة الدولية للتربية والعلوم والثقافة \" اليونسكو \"�\n"
     ]
    }
   ],
   "source": [
    "outputs = generator(prompt, max_length=120, num_return_sequences=3, temperature=0.9, top_p=0.95)\n",
    "for i, out in enumerate(outputs):\n",
    "    print(f\"\\nSample {i+1}:\\n{out['generated_text']}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNe4Ffj0d2Oce0ZHvlsrRJa",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
